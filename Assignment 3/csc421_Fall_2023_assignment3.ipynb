{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Fall 2023 Assignment 3 \n",
    "### Author: George Tzanetakis \n",
    "\n",
    "This notebook is based on the topics covered in **Chapter 12 - Quantifying Uncertainty **, **Chapter 13 Probabilistic Reasoning** , and **Chapter 14 Probabistic Reasoning over time** from the book *Artificial Intelligence: A Modern Approach.*  You are welcome and actually it can be educational to look at the code at the aima-code repository as well as other code resources you can find on the web. However, make sure you understand any code that you incoporate. \n",
    "\n",
    "The assignment structure is as follows - each item is worth 1 point: \n",
    "\n",
    "1. Non-transitive dice war (Basic)   \n",
    "2. Specifying a Bayesian Network (Basic) \n",
    "3. Inference on a Bayesian Netowrk (Expected) \n",
    "4. Specifying a Hidden Markov Model (Basic) \n",
    "5. Inference and estimation using HMM (Expected) \n",
    "6. Text Categorization setup (Basic) \n",
    "7. Text Classification (Expected) \n",
    "8. Text Classification evaluation (Expected) \n",
    "9. Maximum likelihood, MAP, and Baysian Learning for discrete/continuous model \n",
    "10. 2D HMM with Gaussian Mixture Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (Basic)  Non-transitive dice war \n",
    "\n",
    "In this question we will explore generating samples of discrete random variables and the fascinating concept of non-transitive dice. A dice war is a game in which two dice with \n",
    "different probability distributions are randomly sampled, the corresponding \n",
    "samples are comprared, and the one with the highest number is counted as a \"win\". If a dice $A$ wins on average more than half of the time against another dice $B$ we say that dice $A$ beats dice $B$. \n",
    "\n",
    "For example let's consider a standard dice $A$ with probability distribution: $P(A) = <\\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}>$ and a dice $B$ that has 3 faces with the number $3$ and three faces with the number $4$ so that $P(B) = <0, 0, 0, \\frac{3}{6}, \\frac{3}{6}, 0>$.  \n",
    "\n",
    "In the cell below code is provided for defining a random variables by providing an array of values and an array of corresponding probabilities. Complete the function $dice_war$ based on the code and documentation provided in the cell bellow. Show that the dice $B$ described above wins on average the war against dice $A$. \n",
    "\n",
    "Now consider the following three dice/random variables $Red, Green, Blue$. They all have six faces \n",
    "but different values. The corresponding values are: \n",
    "\n",
    "1. $[2,2,4,4,9,9]$ for $Red$\n",
    "2. $[1,1,6,6,8,8]$ for $Green$ \n",
    "3. $[3,3,5,5,7,7]$ for $Blue$\n",
    "\n",
    "Using the dice war function you wrote show the counter-intuitive result that the $Red$ die beats the $Green$ die, the $Green$ die beats the $Blue$ die, but the $Blue$ die beats the $Red$ die. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this code as needed \n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class Random_Variable: \n",
    "    \n",
    "    def __init__(self, name, values, probability_distribution): \n",
    "        self.name = name \n",
    "        self.values = values \n",
    "        self.probability_distribution = probability_distribution\n",
    "        if all(type(item) is np.int64 for item in values):\n",
    "            self.type = 'numeric'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (values, probability_distribution))\n",
    "        elif all(type(item) is str for item in values): \n",
    "            self.type = 'symbolic'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (np.arange(len(values)), probability_distribution))\n",
    "            self.symbolic_values = values \n",
    "        else: \n",
    "            self.type = 'undefined'\n",
    "            \n",
    "    def sample(self,size): \n",
    "        if (self.type =='numeric'):\n",
    "            return self.rv.rvs(size=size)\n",
    "        elif (self.type == 'symbolic'): \n",
    "            numeric_samples = self.rv.rvs(size=size)\n",
    "            mapped_samples = [self.values[x] for x in numeric_samples]\n",
    "            return mapped_samples\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "values = np.array([1,2,3,4,5,6])\n",
    "probabilities_A = np.array([1/6., 1/6., 1/6., 1/6., 1/6., 1/6.])\n",
    "probabilities_B = np.array([0.0, 0.0, 0/6., 3/6., 3/6., 0/6.])\n",
    "\n",
    "dieA = Random_Variable('DieA', values, probabilities_A)\n",
    "dieB = Random_Variable('DieB', values, probabilities_B)\n",
    "\n",
    "\n",
    "\"\"\"Gets and prints the spreadsheet's header columns\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "file_loc : str\n",
    "    The file location of the spreadsheet\n",
    "print_cols : bool, optional\n",
    "    A flag used to print the columns to the console (default is False)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "list\n",
    "    a list of strings representing the header columns\n",
    "\"\"\"\n",
    "\n",
    "def dice_war(A,B, num_samples = 1000, output=True):\n",
    "    # your code goes here \n",
    "    \n",
    "    res = prob > 0.5 \n",
    "    \n",
    "    if output: \n",
    "        if res:\n",
    "            print('{} beats {} with probability {}'.format(A.get_name(),\n",
    "                                                           B.get_name(),\n",
    "                                                           prob))\n",
    "        else:\n",
    "            print('{} beats {} with probability {:.2f}'.format(B.get_name(),\n",
    "                                                               A.get_name(),\n",
    "                                                               1.0-prob))\n",
    "    return (res, prob)\n",
    "        \n",
    "\n",
    "# Add code here to show the non-transitive nature of Red, Green, Blue dice \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (Basic)  - Specifying a Bayesian Network \n",
    "\n",
    "<img src=\"dispnea.png\">\n",
    "\n",
    "Using the convetions for DBNs used in probability.ipynb (from the AIMA authors) encode the diapnea network shown above. Once you have constructed the Bayesian network display the cpt for the Lung Cancer Node (using the API provided not just showing the numbers).\n",
    "\n",
    "The cell below contains the code that defines BayesNodes and BayesNetworks and the following cell \n",
    "shows an example of defining the Burglary network and performing a query using direct enumeration and rejection sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "\n",
    "def extend(s, var, val):\n",
    "    \"\"\"Copy dict s and extend it by setting var to val; return copy.\"\"\"\n",
    "    return {**s, var: val}\n",
    "\n",
    "def event_values(event, variables):                                                                      \n",
    "    \"\"\"Return a tuple of the values of variables in event.                                               \n",
    "    >>> event_values ({'A': 10, 'B': 9, 'C': 8}, ['C', 'A'])                                             \n",
    "    (8, 10)                                                                                              \n",
    "    >>> event_values ((1, 2), ['C', 'A'])                                                                \n",
    "    (1, 2)                                                                                               \n",
    "    \"\"\"                                                                                                  \n",
    "    if isinstance(event, tuple) and len(event) == len(variables):                                        \n",
    "        return event                                                                                     \n",
    "    else:                                                                                                \n",
    "        return tuple([event[var] for var in variables])                                                  \n",
    "                      \n",
    "def probability(p):                                                                                      \n",
    "    \"\"\"Return true with probability p.\"\"\"                                                                \n",
    "    return p > random.uniform(0.0, 1.0)  \n",
    "        \n",
    "class ProbDist:\n",
    "    \"\"\"A discrete probability distribution. You name the random variable\n",
    "    in the constructor, then assign and query probability of values.\n",
    "    >>> P = ProbDist('Flip'); P['H'], P['T'] = 0.25, 0.75; P['H']\n",
    "    0.25\n",
    "    >>> P = ProbDist('X', {'lo': 125, 'med': 375, 'hi': 500})\n",
    "    >>> P['lo'], P['med'], P['hi']\n",
    "    (0.125, 0.375, 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var_name='?', freq=None):\n",
    "        \"\"\"If freq is given, it is a dictionary of values - frequency pairs,\n",
    "        then ProbDist is normalized.\"\"\"\n",
    "        self.prob = {}\n",
    "        self.var_name = var_name\n",
    "        self.values = []\n",
    "        if freq:\n",
    "            for (v, p) in freq.items():\n",
    "                self[v] = p\n",
    "            self.normalize()\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        \"\"\"Given a value, return P(value).\"\"\"\n",
    "        try:\n",
    "            return self.prob[val]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def __setitem__(self, val, p):\n",
    "        \"\"\"Set P(val) = p.\"\"\"\n",
    "        if val not in self.values:\n",
    "            self.values.append(val)\n",
    "        self.prob[val] = p\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Make sure the probabilities of all values sum to 1.\n",
    "        Returns the normalized distribution.\n",
    "        Raises a ZeroDivisionError if the sum of the values is 0.\"\"\"\n",
    "        total = sum(self.prob.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            for val in self.prob:\n",
    "                self.prob[val] /= total\n",
    "        return self\n",
    "\n",
    "    def show_approx(self, numfmt='{:.3g}'):\n",
    "        \"\"\"Show the probabilities rounded and sorted by key, for the\n",
    "        sake of portable doctests.\"\"\"\n",
    "        return ', '.join([('{}: ' + numfmt).format(v, p) for (v, p) in sorted(self.prob.items())])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"P({})\".format(self.var_name)\n",
    "\n",
    "\n",
    "class BayesNode:\n",
    "    \"\"\"A conditional probability distribution for a boolean variable,\n",
    "    P(X | parents). Part of a BayesNet.\"\"\"\n",
    "\n",
    "    def __init__(self, X, parents, cpt):\n",
    "        \"\"\"X is a variable name, and parents a sequence of variable\n",
    "        names or a space-separated string. cpt, the conditional\n",
    "        probability table, takes one of these forms:\n",
    "\n",
    "        * A number, the unconditional probability P(X=true). You can\n",
    "          use this form when there are no parents.\n",
    "\n",
    "        * A dict {v: p, ...}, the conditional probability distribution\n",
    "          P(X=true | parent=v) = p. When there's just one parent.\n",
    "\n",
    "        * A dict {(v1, v2, ...): p, ...}, the distribution P(X=true |\n",
    "          parent1=v1, parent2=v2, ...) = p. Each key must have as many\n",
    "          values as there are parents. You can use this form always;\n",
    "          the first two are just conveniences.\n",
    "\n",
    "        In all cases the probability of X being false is left implicit,\n",
    "        since it follows from P(X=true).\n",
    "\n",
    "        >>> X = BayesNode('X', '', 0.2)\n",
    "        >>> Y = BayesNode('Y', 'P', {T: 0.2, F: 0.7})\n",
    "        >>> Z = BayesNode('Z', 'P Q',\n",
    "        ...    {(T, T): 0.2, (T, F): 0.3, (F, T): 0.5, (F, F): 0.7})\n",
    "        \"\"\"\n",
    "        if isinstance(parents, str):\n",
    "            parents = parents.split()\n",
    "\n",
    "        # We store the table always in the third form above.\n",
    "        if isinstance(cpt, (float, int)):  # no parents, 0-tuple\n",
    "            cpt = {(): cpt}\n",
    "        elif isinstance(cpt, dict):\n",
    "            # one parent, 1-tuple\n",
    "            if cpt and isinstance(list(cpt.keys())[0], bool):\n",
    "                cpt = {(v,): p for v, p in cpt.items()}\n",
    "\n",
    "        assert isinstance(cpt, dict)\n",
    "        for vs, p in cpt.items():\n",
    "            assert isinstance(vs, tuple) and len(vs) == len(parents)\n",
    "            assert all(isinstance(v, bool) for v in vs)\n",
    "            assert 0 <= p <= 1\n",
    "\n",
    "        self.variable = X\n",
    "        self.parents = parents\n",
    "        self.cpt = cpt\n",
    "        self.children = []\n",
    "\n",
    "    def p(self, value, event):\n",
    "        \"\"\"Return the conditional probability\n",
    "        P(X=value | parents=parent_values), where parent_values\n",
    "        are the values of parents in event. (event must assign each\n",
    "        parent a value.)\n",
    "        >>> bn = BayesNode('X', 'Burglary', {T: 0.2, F: 0.625})\n",
    "        >>> bn.p(False, {'Burglary': False, 'Earthquake': True})\n",
    "        0.375\"\"\"\n",
    "        assert isinstance(value, bool)\n",
    "        ptrue = self.cpt[event_values(event, self.parents)]\n",
    "        return ptrue if value else 1 - ptrue\n",
    "\n",
    "    def sample(self, event):\n",
    "        \"\"\"Sample from the distribution for this variable conditioned\n",
    "        on event's values for parent_variables. That is, return True/False\n",
    "        at random according with the conditional probability given the\n",
    "        parents.\"\"\"\n",
    "        return probability(self.p(True, event))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.variable, ' '.join(self.parents)))\n",
    "    \n",
    "    \n",
    "class BayesNet:\n",
    "    \"\"\"Bayesian network containing only boolean-variable nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, node_specs=None):\n",
    "        \"\"\"Nodes must be ordered with parents before children.\"\"\"\n",
    "        self.nodes = []\n",
    "        self.variables = []\n",
    "        node_specs = node_specs or []\n",
    "        for node_spec in node_specs:\n",
    "            self.add(node_spec)\n",
    "\n",
    "    def add(self, node_spec):\n",
    "        \"\"\"Add a node to the net. Its parents must already be in the\n",
    "        net, and its variable must not.\"\"\"\n",
    "        node = BayesNode(*node_spec)\n",
    "        assert node.variable not in self.variables\n",
    "        assert all((parent in self.variables) for parent in node.parents)\n",
    "        self.nodes.append(node)\n",
    "        self.variables.append(node.variable)\n",
    "        for parent in node.parents:\n",
    "            self.variable_node(parent).children.append(node)\n",
    "\n",
    "    def variable_node(self, var):\n",
    "        \"\"\"Return the node for the variable named var.\n",
    "        >>> burglary.variable_node('Burglary').variable\n",
    "        'Burglary'\"\"\"\n",
    "        for n in self.nodes:\n",
    "            if n.variable == var:\n",
    "                return n\n",
    "        raise Exception(\"No such variable: {}\".format(var))\n",
    "\n",
    "    def variable_values(self, var):\n",
    "        \"\"\"Return the domain of var.\"\"\"\n",
    "        return [True, False]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BayesNet({0!r})'.format(self.nodes)\n",
    "    \n",
    "    \n",
    "def enumerate_all(variables, e, bn):\n",
    "    \"\"\"Return the sum of those entries in P(variables | e{others})\n",
    "    consistent with e, where P is the joint distribution represented\n",
    "    by bn, and e{others} means e restricted to bn's other variables\n",
    "    (the ones other than variables). Parents must precede children in variables.\"\"\"\n",
    "    if not variables:\n",
    "        return 1.0\n",
    "    Y, rest = variables[0], variables[1:]\n",
    "    Ynode = bn.variable_node(Y)\n",
    "    if Y in e:\n",
    "        return Ynode.p(e[Y], e) * enumerate_all(rest, e, bn)\n",
    "    else:\n",
    "        return sum(Ynode.p(y, e) * enumerate_all(rest, extend(e, Y, y), bn)\n",
    "                   for y in bn.variable_values(Y))\n",
    "\n",
    "def enumeration_ask(X, e, bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.9]\n",
    "    Return the conditional probability distribution of variable X\n",
    "    given evidence e, from BayesNet bn.\n",
    "    >>> enumeration_ask('Burglary', dict(JohnCalls=T, MaryCalls=T), burglary\n",
    "    ...  ).show_approx()\n",
    "    'False: 0.716, True: 0.284'\"\"\"\n",
    "    assert X not in e, \"Query variable must be distinct from evidence\"\n",
    "    Q = ProbDist(X)\n",
    "    for xi in bn.variable_values(X):\n",
    "        Q[xi] = enumerate_all(bn.variables, extend(e, X, xi), bn)\n",
    "    return Q.normalize()\n",
    "\n",
    "def consistent_with(event, evidence):\n",
    "    \"\"\"Is event consistent with the given evidence?\"\"\"\n",
    "    return all(evidence.get(k, v) == v for k, v in event.items())\n",
    "\n",
    "def prior_sample(bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.13]\n",
    "    Randomly sample from bn's full joint distribution.\n",
    "    The result is a {variable: value} dict.\n",
    "    \"\"\"\n",
    "    event = {}\n",
    "    for node in bn.nodes:\n",
    "        event[node.variable] = node.sample(event)\n",
    "    return event\n",
    "\n",
    "def rejection_sampling(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.14]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn, using N samples.\n",
    "    Raises a ZeroDivisionError if all the N samples are rejected,\n",
    "    i.e., inconsistent with e.\n",
    "    >>> random.seed(47)\n",
    "    >>> rejection_sampling('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.7, True: 0.3'\n",
    "    \"\"\"\n",
    "    counts = {x: 0 for x in bn.variable_values(X)}  # bold N in [Figure 14.14]\n",
    "    for j in range(N):\n",
    "        sample = prior_sample(bn)  # boldface x in [Figure 14.14]\n",
    "        if consistent_with(sample, e):\n",
    "            counts[sample[X]] += 1\n",
    "    return ProbDist(X, counts)\n",
    "\n",
    "def weighted_sample(bn, e):\n",
    "    \"\"\"\n",
    "    Sample an event from bn that's consistent with the evidence e;\n",
    "    return the event and its weight, the likelihood that the event\n",
    "    accords to the evidence.\n",
    "    \"\"\"\n",
    "    w = 1\n",
    "    event = dict(e)  # boldface x in [Figure 14.15]\n",
    "    for node in bn.nodes:\n",
    "        Xi = node.variable\n",
    "        if Xi in e:\n",
    "            w *= node.p(e[Xi], event)\n",
    "        else:\n",
    "            event[Xi] = node.sample(event)\n",
    "    return event, w\n",
    "\n",
    "def likelihood_weighting(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.15]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn.\n",
    "    >>> random.seed(1017)\n",
    "    >>> likelihood_weighting('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.702, True: 0.298'\n",
    "    \"\"\"\n",
    "    W = {x: 0 for x in bn.variable_values(X)}\n",
    "    for j in range(N):\n",
    "        sample, weight = weighted_sample(bn, e)  # boldface x, w in [Figure 14.15]\n",
    "        W[sample[X]] += weight\n",
    "    return ProbDist(X, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({False: 8004, True: 1996})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "bn = BayesNode('X', 'Burglary', {True: 0.2, False: 0.625})\n",
    "\n",
    "bn.p(True, {'Burglary': False, 'Earthquake': True})\n",
    "\n",
    "samples = [] \n",
    "for i in range(0,10000): \n",
    "    samples.append(bn.sample({'Burglary': True, 'Earthquake': True}))\n",
    "print(Counter(samples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}\n",
      "0.2841718353643929 0.7158281646356071\n",
      "False: 0.667, True: 0.333\n"
     ]
    }
   ],
   "source": [
    "burglary = BayesNet([\n",
    "        ('Burglary', '', 0.001),\n",
    "        ('Earthquake', '', 0.002),\n",
    "        ('Alarm', ['Burglary', 'Earthquake'],\n",
    "         {(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}),\n",
    "        ('JohnCalls', 'Alarm', {True: 0.90, False: 0.05}),\n",
    "        ('MaryCalls', 'Alarm', {True: 0.70, False: 0.01})\n",
    "    ])\n",
    "print(burglary.variable_node('Alarm').cpt)\n",
    "ans_dist = enumeration_ask('Burglary', {'JohnCalls': True, 'MaryCalls': True}, burglary)\n",
    "print(ans_dist[True],ans_dist[False])\n",
    "print(rejection_sampling('Burglary', dict(JohnCalls=True, MaryCalls=True), burglary, 10000).show_approx())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 (Expected) - Querying the Baysian Network \n",
    "\n",
    "Answer using exact inference with enumeration the following query: given that a patient has been in Asia and has a positive xray, what is the likelihood of having dispnea using enumeration ask \n",
    "Write code for this same query using enumeration_ask. Answer using approximate inference the same query using both rejection sampling and likelihood weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Question 4 (Basic) - Generate HMM samples \n",
    "\n",
    "\n",
    "The next three question explore hidden markov models (HMMs) and use the hmmlearn Python library:\n",
    "[https://hmmlearn.readthedocs.io/en/latest/]. \n",
    "\n",
    "You can use the code for the weather example in the probabilistic reasoning over time notebook we covered in class as a template for writing your code. \n",
    "\n",
    "The problem used in inspired by the use of HMMs in bioinformatics. There are several simplifications made to make it reasonable as part of an assignment. DNA sequences can be considered strings over an alphabet of 4 symbols/nucleobases **A,C,T,G (adenine, cytosine, thymine, guanine**. Parts of a DNA sequence are dense with C and G and other parts are sparse with C and G and it is of interest to biologists to identify these regions. \n",
    "\n",
    "We will model the **CG-dense**(CGD) and **CG-sparse** (CGS) as hidden states and the nucleobases are the observations. Through experimental data we have the following information: \n",
    "\n",
    "1. The transition probability from CGD to CGS is 0.37 and the probability of staying in CGD is 0.63. The transition probability from CGS to CGD is similarly 0.37 with 0.63 being the probability of staying in CGS. \n",
    "\n",
    "2. The observation probabilities of CGD regions are: A: 0.15, C:0.35, G: 0.35, and T:0.15. The observation probabilities of CGS regions are: A: 0.40, C: 0.10, G: 0.10, T: 0.40 \n",
    "\n",
    "3. You can assume that the initial state probabilities are the same (0.5) \n",
    "\n",
    "4. For visualization of the DNA sequences use the following color mapping: A: red, C: green, T: blue, G: yellow, and for CGD: black \n",
    "and CGS: white \n",
    "\n",
    "\n",
    "Define this HMM model using the **hmmlearn** conventions. Then use the created model to generate a sequence of 1000 samples (i.e both hidden states and corresponding observations). Use the colors above to visualize the sequence of samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (Expected) - Learning the HMM model from data \n",
    "\n",
    "Generate 10000 samples using the defined hmm for generating DNA sequences. Learn the HMM in an unsupervised fashion similarly to what we did with the weather example i.e only use the observation samples not the \"hidden\" states for estimating the model using the fit function. Constrast the original HMM to the HMM estimated from the data by comparing the transition and observation matrices. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 (Basic) - Text categorization setup \n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of categories, on the basis of text it contains. Naive Bayes models are often used for this task. In these models, the query variable is the document category, and the effect variables are the presence/absence of each word in the language; the assumption is that words occur independently in documents within a given category (condititional independence), with frequencies determined by document category. Download the following file: http://www.cs.cornell.edu/People/pabo/movie-review-data/review_polarity.tar.gz containing a dataset that has been used for text mining consisting of movie reviews classified into negative and positive. You will see that there are two folders for the positivie and negative category and they each contain multiple text files with the reviews. You can find more information about the dataset at: http://www.cs.cornell.edu/People/pabo/movie-review-data/\n",
    "\n",
    "Our goal will be to build a simple Naive Bayes classifier for this dataset. More complicated approaches using term frequency and inverse document frequency weighting and many more words are possible but the basic concepts are the same. The goal is to understand the whole process so DO NOT use existing machine learning packages but rather build the classifier from scratch.\n",
    "\n",
    "Our feature vector representation for each text file will be simply a binary vector that shows which of the following words are present in the text file: Awful Bad Boring Dull Effective Enjoyable Great Hilarious. For example the text file cv996 11592.txt would be represented as (0, 0, 0, 0, 1, 0, 1, 0) because it contains Effective and Great but none of the other words.\n",
    "\n",
    "Write code that parses the text files and calculates probabilities for each dictionary word given the reivew polarity. You can use any of the code provided in the notebook on quantifying uncertainty related to text Naive Bayes Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 7 (EXPECTED) - Text classification  \n",
    "\n",
    "Write Python code for classifying a particular test instance (in our case movie review) following a Bernoulli Bayes approach i.e you just model the presence/absence of each word. Your code should calculate the likelihood the review is positive given the correspondng conditional probabilities for each dictionary word as well as the likelihood the review is negative given the corresponding conditional probabilities for each dictionary word. Check that your code works by providing a few example cases of prediction. Your code should be written from \"scratch\" and only use numpy/scipy but **NOT** machine learning libraries like scikit-learn or tensorflow or pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 8 (EXPECTED) - Text classification evaluation \n",
    "\n",
    "Calculate the classification accuracy and confusion matrix that you would obtain using the whole data set for both training and testing. Do not use machine learning libraries like scikit-learn or tensorflow for this only the basic numpy/scipy stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 9 (ADVANCED) - Maximum Likelihood, MAP, and Bayesian Learning \n",
    "\n",
    "In this question you are asked to come up with a variation to an existing example from the book. \n",
    "The example was covered in the notebook about learning probabilistic models and describes 5 bags of candy of two types (lime, cherry) as 5 hypotheses. Come up with a scenario (be creative but realistic) in which each hypothesis is similarly selectected with a prior probability but instead of a discrete probability distribution of lime and cherry candies as in the original example, you use a continuous probability density function fir each hypothesis. You can use simple Normal distributions characterized by a mean and variance for each hypothesis. Your example should describe a realistic scenario i.e not just have numbers but have some kind of interpretation. \n",
    "\n",
    "Show in a similar fashion to how the example is described in the book how you would perform maximum likelihood estimation, maximum a posteriori estimation, and Baysian Learning given some successive samples from your model. Include a plot in your example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 10 (ADVANCED) - HMM for 2D Location Estimation \n",
    "\n",
    "This question is a bit more open ended, will require some creativity and extra work. Consider the following problem: during your day your cell phone collects location data in terms of x,y coordinates. You do different activities such as going to university, eating, going to the gym. These activities take place in particular locations such as Restaurant A and Restaurant B or Gym A, Gym B and each particular location can be thought of as a two-dimensional Gaussian distribution of location points (your GPS sensor has some noise and depending on \n",
    "the time you are at different parts of the corresponding building). If you consider the activity as the hidden state and the location as the observation you have a Hidden Markov Model. Because activities take place in multiple locations you can model this as a Gaussian Mixture Model (GMM). Each Gaussian will be multivariate 2D Gaussian distribution characterized by two means and and a 2 by 2 covariance matrix.\n",
    "\n",
    "Consider a hypothetical scenario with 3 activities (eat, study, exercise) and 3 locations (GMM components) for each activity. You will need to do some reading about how GMMs work. You can come up with reasonable estimates for the associated parameters.\n",
    "\n",
    "Basically the goal is the follow the format of the Markov Chain and HMM notebook and create appropriate visualizations using this problem.\n",
    "\n",
    "Visualize on a 2D plane using circles the different locations and corresponding mixture components \n",
    "\n",
    "1. Generate a dataset using a Hidden Markov Model of the problem \n",
    "2. Visualize the dataset on a 2D plane \n",
    "3. Show how you can learn the parameters of this HMM using https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GMMHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
